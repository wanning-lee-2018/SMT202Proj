{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Through topic modelling, we want to find out the common topics derived from people's negative sentiments regarding public spaces in Paya Lebar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard dataframe packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Text analytics packages\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: Try either of the following encodings: encoding='utf-8' or encoding='ISO-8859-1'\n",
    "#import data\n",
    "data=pd.read_excel('Sentiment.xlsx',sheet_name=\"Main\",skipfooter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Social Offerings</th>\n",
       "      <th>Openness</th>\n",
       "      <th>Amenities</th>\n",
       "      <th>Cultural Heritage</th>\n",
       "      <th>Full text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are staying in Geylang near the river. We g...</td>\n",
       "      <td>Very nice. So far I haven't encountered bad pe...</td>\n",
       "      <td>I think it’s already nice just the mosquitoes....</td>\n",
       "      <td>Usually we will go to the mall. I don’t get to...</td>\n",
       "      <td>We are staying in Geylang near the river. We g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not so really bonded to community. Paya Lebar ...</td>\n",
       "      <td>Last time my impression was this place was for...</td>\n",
       "      <td>The most modern building will be Paya Lebar Qu...</td>\n",
       "      <td>Yeah then the URA got a certain development pl...</td>\n",
       "      <td>Not so really bonded to community. Paya Lebar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OK actually we go to the shopping mall. This i...</td>\n",
       "      <td>Oh yeah absolutely absolutely welcome people i...</td>\n",
       "      <td>Let's say I was very surprised by PLQ. I think...</td>\n",
       "      <td>Absolutely important, yes that's why I think P...</td>\n",
       "      <td>OK actually we go to the shopping mall. This i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I started liking this place after 4 months sta...</td>\n",
       "      <td>Around the condo. Just like family, we know ea...</td>\n",
       "      <td>Playground one, two things very boring. I thin...</td>\n",
       "      <td>Don't really know any cultural heritage here. ...</td>\n",
       "      <td>I started liking this place after 4 months sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My house is near to church. I’m a christian so...</td>\n",
       "      <td>I feel connected because mostly food. I usuall...</td>\n",
       "      <td>The river reminds me of my hometown.</td>\n",
       "      <td>No I don’t go there.</td>\n",
       "      <td>My house is near to church. I’m a christian so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Social Offerings  \\\n",
       "0  We are staying in Geylang near the river. We g...   \n",
       "1  Not so really bonded to community. Paya Lebar ...   \n",
       "2  OK actually we go to the shopping mall. This i...   \n",
       "3  I started liking this place after 4 months sta...   \n",
       "4  My house is near to church. I’m a christian so...   \n",
       "\n",
       "                                            Openness  \\\n",
       "0  Very nice. So far I haven't encountered bad pe...   \n",
       "1  Last time my impression was this place was for...   \n",
       "2  Oh yeah absolutely absolutely welcome people i...   \n",
       "3  Around the condo. Just like family, we know ea...   \n",
       "4  I feel connected because mostly food. I usuall...   \n",
       "\n",
       "                                           Amenities  \\\n",
       "0  I think it’s already nice just the mosquitoes....   \n",
       "1  The most modern building will be Paya Lebar Qu...   \n",
       "2  Let's say I was very surprised by PLQ. I think...   \n",
       "3  Playground one, two things very boring. I thin...   \n",
       "4              The river reminds me of my hometown.    \n",
       "\n",
       "                                   Cultural Heritage  \\\n",
       "0  Usually we will go to the mall. I don’t get to...   \n",
       "1  Yeah then the URA got a certain development pl...   \n",
       "2  Absolutely important, yes that's why I think P...   \n",
       "3  Don't really know any cultural heritage here. ...   \n",
       "4                              No I don’t go there.    \n",
       "\n",
       "                                           Full text  \n",
       "0  We are staying in Geylang near the river. We g...  \n",
       "1  Not so really bonded to community. Paya Lebar ...  \n",
       "2  OK actually we go to the shopping mall. This i...  \n",
       "3  I started liking this place after 4 months sta...  \n",
       "4  My house is near to church. I’m a christian so...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick peek at the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data.loc[:,\"Full text\"].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import external packages for sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "#quick peek at data generated from vader\n",
    "# for i in range(len(data2)):\n",
    "#     scores = analyzer.polarity_scores(data2.iloc[i])\n",
    "#     print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER produces four sentiment metrics from these word ratings The first three, positive, neutral and negative,\n",
    "# represent the proportion of the text that falls into those categories. \n",
    "# The polarity_scores gives us numerical values for use of negative, neutral, and positive word choice. \n",
    "# The compound value reflects the overall sentiment normalized to range of -1 being very negative to +1 being very positive.\n",
    "#Printing the sentiment nicely in a table format. \n",
    "my_vader_score_compound = [ ] \n",
    "my_vader_score_positive = [ ] \n",
    "my_vader_score_negative = [ ] \n",
    "my_vader_score_neutral = [ ] \n",
    "\n",
    "for i in range(len(data2)):\n",
    "    my_analyzer = analyzer.polarity_scores(data2.iloc[i])\n",
    "    my_vader_score_compound.append(my_analyzer['compound'])\n",
    "    my_vader_score_positive.append(my_analyzer['pos'])\n",
    "    my_vader_score_negative.append(my_analyzer['neg']) \n",
    "    my_vader_score_neutral.append(my_analyzer['neu']) \n",
    "\n",
    "\n",
    "#converting sentiment values to numpy for easier usage\n",
    "my_vader_score_compound = np.array(my_vader_score_compound)\n",
    "my_vader_score_positive = np.array(my_vader_score_positive)\n",
    "my_vader_score_negative = np.array(my_vader_score_negative)\n",
    "my_vader_score_neutral = np.array(my_vader_score_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataVader=data2.copy()\n",
    "dataVader=pd.DataFrame(dataVader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full text</th>\n",
       "      <th>Score</th>\n",
       "      <th>postve</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are staying in Geylang near the river. We g...</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not so really bonded to community. Paya Lebar ...</td>\n",
       "      <td>0.9886</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OK actually we go to the shopping mall. This i...</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I started liking this place after 4 months sta...</td>\n",
       "      <td>0.9914</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My house is near to church. I’m a christian so...</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Full text   Score  postve    neg  \\\n",
       "0  We are staying in Geylang near the river. We g...  0.9668   0.184  0.000   \n",
       "1  Not so really bonded to community. Paya Lebar ...  0.9886   0.110  0.036   \n",
       "2  OK actually we go to the shopping mall. This i...  0.9996   0.240  0.012   \n",
       "3  I started liking this place after 4 months sta...  0.9914   0.205  0.022   \n",
       "4  My house is near to church. I’m a christian so...  0.9420   0.190  0.037   \n",
       "\n",
       "     neu  \n",
       "0  0.816  \n",
       "1  0.855  \n",
       "2  0.748  \n",
       "3  0.773  \n",
       "4  0.773  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataVader['Score'] = my_vader_score_compound\n",
    "dataVader['postve'] = my_vader_score_positive\n",
    "dataVader['neg'] = my_vader_score_negative\n",
    "dataVader['neu'] = my_vader_score_neutral\n",
    "dataVader.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting threshold for what comment is considered negative sentiment\n",
    "neg_data=dataVader[dataVader[\"Score\"]<=0.40]\n",
    "neg_text=neg_data[\"Full text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create stop words list\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.extend(['quite', 'paya', 'lebar', 'PLQ', 'would','go','come','already'])\n",
    "# stop_words.extend(['quite', 'paya', 'lebar', 'PLQ', 'would','go','come',\n",
    "#                    'already','feel','like','place','around','area','this',\n",
    "#                    'know','also','really','nice','enough','good','yeah','important','love','absolutely','still','okay','even','though','cuz'\n",
    "#                   ,'make'])\n",
    "\n",
    "#create tokenizer\n",
    "#\\w matches [a-zA-Z0-9_]. This tokenizer splits the string using regular expressions. E.g - '[A-Z]\\w+' will select just the capital words\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#create lemmatizer\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['many_shopping', 'shopping_mall', 'mall_big', 'big_change', 'change_last', 'last_4', '4_year', 'year_never', 'never_visit', 'visit_realised', 'realised_completely', 'completely_changed', 'changed_cultural', 'cultural_heritage', 'heritage_see']\n",
      "['open_space', 'space_kid', 'kid_move', 'move_play', 'play_around', 'around_compared', 'compared_river', 'river_valley', 'valley_convenience', 'convenience_ecp', 'ecp_know', 'know_cultural', 'cultural_heritage']\n",
      "['face_slight', 'slight_discrimination', 'discrimination_enter', 'enter_shopping', 'shopping_mall', 'mall_ask', 'ask_check', 'check_id', 'id_go', 'go_gym', 'gym_people', 'people_talk', 'talk_behind', 'behind_back', 'back_ask', 'ask_away', 'away_much', 'much_amenity', 'amenity_sport', 'sport_proper', 'proper_running', 'running_track', 'track_apart', 'apart_nearby', 'nearby_indoor', 'indoor_gym', 'gym_know', 'know_cultural', 'cultural_heritage']\n",
      "['difficult_take', 'take_rest', 'rest_outside', 'outside_kopitiam', 'kopitiam_settle', 'settle_public', 'public_area', 'area_sometimes', 'sometimes_regulation', 'regulation_strict', 'strict_cannot', 'cannot_eat', 'eat_cannot', 'cannot_cross', 'cross_leg', 'leg_cannot', 'cannot_talk', 'talk_loudly', 'loudly_cannot', 'cannot_bring', 'bring_heavy', 'heavy_food', 'food_eat', 'eat_know', 'know_cultural', 'cultural_heritage']\n",
      "['crowded_large', 'large_variety', 'variety_food', 'food_option', 'option_drinking', 'drinking_place', 'place_gym', 'gym_exercise', 'exercise_greenery', 'greenery_refreshing', 'refreshing_high', 'high_rise', 'rise_ceiling', 'ceiling_make', 'make_le', 'le_cooped', 'cooped_stand', 'stand_rest', 'rest_new', 'new_modern', 'modern_compared', 'compared_seeing', 'seeing_hdb', 'hdb_flat', 'flat_geylang', 'geylang_serai', 'serai_know']\n",
      "['singpost_see', 'see_movie', 'movie_monday', 'monday_sunday', 'sunday_shop', 'shop_last', 'last_time', 'time_sunday', 'sunday_family', 'family_day', 'day_family', 'family_day', 'day_mostly', 'mostly_singpost', 'singpost_good', 'good_family', 'family_bonding', 'bonding_uncle', 'uncle_time', 'time_organised', 'organised_pap', 'pap_cc', 'cc_thing', 'thing_went', 'went_help', 'help_uncle', 'uncle_passed', 'passed_away', 'away_everything', 'everything_stayed', 'stayed_joo', 'joo_chiat', 'chiat_complex', 'complex_uncle', 'uncle_passed', 'passed_away', 'away_never', 'never_see', 'see_neighbour', 'neighbour_morning', 'morning_6', '6_something', 'something_back', 'back_12', '12_child', 'child_play', 'play_child', 'child_video', 'video_game', 'game_handphones', 'handphones_house', 'house_kid', 'kid_got', 'got_play', 'play_football', 'football_never', 'never_see', 'see_park', 'park_east', 'east_coast', 'coast_park', 'park_lockdown', 'lockdown_also', 'also_hard', 'hard_idea', 'idea_glenlink', 'glenlink_square', 'square_kampung', 'kampung_joo', 'joo_chiat', 'chiat_complex', 'complex_uncle', 'uncle_place', 'place_always', 'always_play', 'play_seesaw', 'seesaw_carpark', 'carpark_football', 'football_field', 'field_army', 'army_friend', 'friend_visit', 'visit_shop', 'shop_lot', 'lot_changing', 'changing_miss', 'miss_past', 'past_important', 'important_child', 'child_know', 'know_heritage', 'heritage_hawker', 'hawker_line', 'line_generation', 'generation_gone', 'gone_kid', 'kid_cannot', 'cannot_play', 'play_game', 'game_one', 'one_thirteen', 'thirteen_one', 'one_ten', 'ten_year', 'year_old', 'old_old', 'old_geylang', 'geylang_serai', 'serai_market', 'market_stopped', 'stopped_school', 'school_primary', 'primary_6', '6_uncle', 'uncle_shop', 'shop_always', 'always_helped', 'helped_old', 'old_geylang', 'geylang_serai', 'serai_market', 'market_city', 'city_plaza', 'plaza_remember', 'remember_mum', 'mum_father', 'father_working', 'working_salon', 'salon_shop', 'shop_second', 'second_floor', 'floor_anolas', 'anolas_nice', 'nice_chicken', 'chicken_small', 'small_memory', 'memory_tanjong', 'tanjong_katong', 'katong_hari', 'hari_raya', 'raya_clothes', 'clothes_nothing', 'nothing_canal', 'canal_played', 'played_canal', 'canal_swim', 'swim_tie', 'tie_string', 'string_jump', 'jump_inside', 'inside_canal', 'canal_last', 'last_time', 'time_playground', 'playground_fishing', 'fishing_pod', 'pod_last', 'last_time', 'time_dirty', 'dirty_thing', 'thing_river', 'river_went', 'went_back', 'back_flat', 'flat_became', 'became_sick', 'sick_getting', 'getting_symptom', 'symptom_sick', 'sick_dirty', 'dirty_river', 'river_going', 'going_sengkang', 'sengkang_place', 'place_nothing', 'nothing_place', 'place_still', 'still_see', 'see_malay', 'malay_culture', 'culture_see', 'see_every', 'every_hari', 'hari_raya', 'raya_malay', 'malay_still', 'still_wearing', 'wearing_baju', 'baju_kurong', 'kurong_hari', 'hari_raya', 'raya_last', 'last_time', 'time_malay', 'malay_village', 'village_still', 'still_see', 'see_old', 'old_malay', 'malay_culture', 'culture_made', 'made_city', 'city_like', 'like_kampung', 'kampung_house', 'house_got', 'got_cc']\n"
     ]
    }
   ],
   "source": [
    "#create bigrams\n",
    "from nltk.util import bigrams\n",
    "\n",
    "def process_bigrams(documents):\n",
    "    process_docs = []\n",
    "    doc_tokens = []\n",
    "    for doc in documents:\n",
    "        doc = doc.lower()\n",
    "        doc_tokens = wtk.tokenize(doc)\n",
    "        doc_tokens = [token for token in doc_tokens if token not in stop_words]\n",
    "        doc_tokens = [wnl.lemmatize(token) for token in doc_tokens] #Can try spacy or see how to use the (nltk.pos_tag()) with lemmatizer\n",
    "        doc_tokens= [\"_\".join(t) for t in bigrams(doc_tokens)]\n",
    "        print(doc_tokens)\n",
    "        doc_tokens = [token for token in doc_tokens if len(token) > 2]\n",
    "        process_docs.append(doc_tokens)\n",
    "    return process_docs\n",
    "process_neg_bigrams=process_bigrams(neg_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents. Each word gets an id\n",
    "dictionary = gensim.corpora.Dictionary(process_neg_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering out bigrams based on their occurrence in the whole sentiments dataset\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming corpus into bag of words vectors\n",
    "corpus_vect = [dictionary.doc2bow(text) for text in process_neg_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train lda model\n",
    "num_topics =3\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus_vect, num_topics = num_topics, id2word=dictionary, passes=25, random_state= 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.016*\"know_cultural\" + 0.009*\"settle_public\" + 0.009*\"strict_cannot\" + 0.009*\"cannot_talk\" + 0.009*\"regulation_strict\" + 0.009*\"cannot_cross\" + 0.009*\"eat_cannot\" + 0.009*\"eat_know\" + 0.009*\"cannot_bring\" + 0.009*\"kopitiam_settle\" + 0.009*\"leg_cannot\" + 0.009*\"heavy_food\" + 0.009*\"talk_loudly\" + 0.009*\"bring_heavy\" + 0.009*\"area_sometimes\" + 0.009*\"cross_leg\" + 0.009*\"difficult_take\"\n",
      "1: 0.011*\"shopping_mall\" + 0.011*\"know_cultural\" + 0.011*\"gym_people\" + 0.011*\"ask_away\" + 0.011*\"ask_check\" + 0.011*\"running_track\" + 0.011*\"back_ask\" + 0.011*\"nearby_indoor\" + 0.011*\"id_go\" + 0.011*\"much_amenity\" + 0.011*\"talk_behind\" + 0.011*\"go_gym\" + 0.011*\"indoor_gym\" + 0.011*\"behind_back\" + 0.011*\"apart_nearby\" + 0.011*\"people_talk\" + 0.011*\"enter_shopping\"\n",
      "2: 0.013*\"last_time\" + 0.010*\"hari_raya\" + 0.010*\"geylang_serai\" + 0.007*\"complex_uncle\" + 0.007*\"family_day\" + 0.007*\"chiat_complex\" + 0.007*\"malay_culture\" + 0.007*\"still_see\" + 0.007*\"passed_away\" + 0.007*\"joo_chiat\" + 0.007*\"never_see\" + 0.007*\"serai_market\" + 0.007*\"old_geylang\" + 0.007*\"uncle_passed\" + 0.004*\"old_old\" + 0.004*\"fishing_pod\" + 0.004*\"past_important\"\n"
     ]
    }
   ],
   "source": [
    "#formatting the topics in print\n",
    "for num ,topic in ldamodel.show_topics(formatted=True, num_topics=num_topics, num_words=17):\n",
    "    print(str(num)+\": \"+ topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  75.63623549360358\n",
      "\n",
      "Coherence Score:  0.7930765307846085\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the performance of trained lda model \n",
    "from gensim.models import CoherenceModel\n",
    "#perplexity score the lower the better\n",
    "log_perplexity = ldamodel.log_perplexity(corpus_vect)\n",
    "perplexity = 2**(-log_perplexity)\n",
    "print('Perplexity: ',perplexity)\n",
    "\n",
    "# Compute Coherence Score\n",
    "#coherence score the higher the better\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=process_neg_bigrams, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
